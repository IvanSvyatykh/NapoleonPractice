{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0aae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Model\n",
    "path_to_conf = \"/home/research/NapoleonPractice/easy_ocr/configs/train_configs/cyrillic_config_v4.yaml\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a7cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from utils import AttrDict,CTCLabelConverter,AttnLabelConverter\n",
    "\n",
    "\n",
    "def get_config(file_path: Path) -> AttrDict:\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == \"None\":\n",
    "        characters = \"\"\n",
    "        for data in opt[\"select_data\"].split(\"-\"):\n",
    "            csv_path = os.path.join(opt[\"train_data\"], data, \"labels.csv\")\n",
    "            df = pd.read_csv(\n",
    "                csv_path,\n",
    "                sep=\"^([^,]+),\",\n",
    "                engine=\"python\",\n",
    "                usecols=[\"filename\", \"words\"],\n",
    "                keep_default_na=False,\n",
    "            )\n",
    "            all_char = \"\".join(df[\"words\"])\n",
    "            characters += \"\".join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character = \"\".join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    os.makedirs(f\"./saved_models/{opt.experiment_name}\", exist_ok=True)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0bd8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(path_to_conf)\n",
    "if \"CTC\" in config.Prediction:\n",
    "    converter = CTCLabelConverter(config.character)\n",
    "else:\n",
    "    converter = AttnLabelConverter(config.character)\n",
    "config.num_class = len(converter.character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19444d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Transformation module specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_161621/1146890810.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(\"/home/research/NapoleonPractice/easy_ocr/saved_models/cyrillic_g2_8000_Adadelta_with_GradScaler/best_accuracy.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Model(\n",
       "    (FeatureExtraction): VGG_FeatureExtractor(\n",
       "      (ConvNet): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): ReLU(inplace=True)\n",
       "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (16): ReLU(inplace=True)\n",
       "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "        (19): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
       "    (SequenceModeling): Sequential(\n",
       "      (0): BidirectionalLSTM(\n",
       "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
       "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): BidirectionalLSTM(\n",
       "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
       "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (Prediction): Linear(in_features=256, out_features=208, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "pretrained_dict = torch.load(\"/home/research/NapoleonPractice/easy_ocr/saved_models/cyrillic_g2_8000_Adadelta_with_GradScaler/best_accuracy.pth\")\n",
    "model = Model(config)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "model.load_state_dict(pretrained_dict, strict=False)  \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778f9285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_root:    ../../data/val\t dataset: /\n",
      "../../data/val/images\n",
      "sub-directory:\t/images\t num samples: 19730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/NapoleonPractice/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "from dataset import AlignCollate, hierarchical_dataset\n",
    "import numpy as np\n",
    "\n",
    "AlignCollate_valid = AlignCollate(\n",
    "        imgH=config.imgH,\n",
    "        imgW=config.imgW,\n",
    "        keep_ratio_with_pad=config.PAD,\n",
    "        contrast_adjust=config.contrast_adjust,\n",
    "    )\n",
    "valid_dataset, valid_dataset_log = hierarchical_dataset(\n",
    "        root=config.valid_data, opt=config\n",
    "    )\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=min(32, config.batch_size),\n",
    "        shuffle=True,  # 'True' to check training progress with validation function.\n",
    "        num_workers=int(config.workers),\n",
    "        prefetch_factor=512,\n",
    "        collate_fn=AlignCollate_valid,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a65c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "img , _ = valid_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f592c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 56)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab19b765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/NapoleonPractice/.venv/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 113])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms.v2 import ToTensor\n",
    "ToTensor()(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef8984eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/NapoleonPractice/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "batch,labels = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5acb19ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 64, 600])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation(model: Model, evaluation_loader, converter, opt):\n",
    "    \"\"\"validation or evaluation\"\"\"\n",
    "\n",
    "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
    "        batch_size = image_tensors.size(0)\n",
    "        image = image_tensors.to(device)\n",
    "        text_for_pred = (\n",
    "            torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "        )\n",
    "\n",
    "        if \"CTC\" in opt.Prediction:\n",
    "            preds = model(image, text_for_pred)\n",
    "            preds = preds.to(\"cpu\")\n",
    "            # Calculate evaluation loss for CTC decoder.\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_index = preds_index.view(-1)\n",
    "            preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "                \n",
    "            print(preds_str)\n",
    "            print(labels)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9576e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31569', '15299', '28900', '43999', '8549', '99999', '10490', '34999', '3290', '11099', '13509', '12999', '7199', '8699', '6999', '1699', '5900', '5749', '28699', '7399', '4150', '12989', '7599', '6599', '3099', '9099', '17999', '3999', '14899', '16699', '11499', '9699']\n",
      "['31569', '15299', '28900', '43999', '8549', '9999', '10490', '34999', '3290', '11099', '13509', '12999', '7199', '8699', '6999', '1699', '5900', '5749', '28699', '7399', '4150', '12989', '7599', '6599', '3099', '9099', '17999', '3999', '14899', '16699', '11499', '9699']\n",
      "['5199', '13589', '11500', '42599', '13350', '4390', '1899', '1299', '24999', '10999', '3499', '13999', '12790', '1279', '19999', '3300', '12499', '7499', '13699', '5800', '6999', '18799', '2800', '9500', '1299', '3699', '4300', '13399', '1999', '33989', '34999', '5200']\n",
      "['5199', '13589', '11500', '42599', '13350', '4390', '1899', '1299', '24999', '10999', '3499', '13999', '12790', '1279', '19999', '3300', '12499', '7499', '13699', '5800', '6999', '18799', '2800', '9500', '1299', '3699', '4300', '13399', '1999', '33989', '34999', '5200']\n",
      "['2849', '699', '1650', '9299', '10699', '21999', '3999', '9499', '18889', '7700', '10500', '4399', '9499', '9699', '6699', '30999', '6199', '2449', '30499', '14990', '4999', '3849', '7149', '26299', '7399', '44999', '8599', '4389', '27999', '11999', '399', '2490']\n",
      "['2849', '699', '1650', '9299', '10699', '21999', '3999', '9499', '18889', '7700', '10500', '4399', '9499', '9699', '6699', '30999', '6199', '2449', '30499', '14990', '4999', '3849', '7149', '26299', '7399', '44999', '8599', '4389', '27999', '11999', '6299', '2490']\n",
      "['10499', '18999', '15999', '12500', '2900', '26989', '9949', '1699', '799', '33999', '6599', '8499', '22990', '39499', '14999', '11499', '11699', '12499', '5689', '3149', '1599', '6200', '1199', '94999', '3799', '22900', '31299', '5099', '25499', '49889', '14499', '11988']\n",
      "['10499', '18999', '15999', '12500', '2900', '26988', '9949', '1699', '799', '33999', '6599', '8499', '22990', '39499', '14999', '11499', '11699', '12499', '5689', '3149', '1599', '6200', '1199', '94999', '3799', '22900', '31299', '5099', '25499', '49889', '14499', '11988']\n",
      "['25989', '8199', '11899', '5699', '12590', '5299', '12599', '6499', '1949', '5579', '9389', '13299', '32299', '8899', '32999', '10499', '28999', '3689', '12699', '3869', '3499', '5499', '7599', '9499', '2799', '18999', '9499', '3289', '15299', '10569', '2084', '3799']\n",
      "['25989', '8199', '11899', '5699', '12590', '5299', '12599', '6499', '1949', '5579', '9389', '13299', '32299', '8899', '32999', '10499', '28999', '3689', '12699', '3869', '3499', '5499', '7599', '9499', '2799', '18999', '9499', '3289', '15299', '10569', '2084', '3799']\n",
      "['15599', '2690', '84999', '17599', '32299', '19900', '12789', '6899', '29299', '16699', '74999', '10299', '3900', '8699', '8999', '6499', '3789', '5399', '5599', '9499', '1650', '6599', '5099', '13289', '4199', '2499', '9299', '3399', '7999', '12599', '8199', '19999']\n",
      "['15599', '2690', '84999', '17599', '32299', '19900', '12789', '6899', '29299', '16699', '74999', '10299', '3900', '8699', '8999', '6499', '3789', '5399', '5599', '9499', '1650', '6599', '5099', '13269', '4199', '2499', '9299', '3399', '7999', '12599', '8199', '19999']\n",
      "['22990', '12999', '6899', '6499', '27999', '33999', '49999', '1299', '7999', '2470', '4299', '17499', '6199', '5019', '9890', '13869', '17299', '47999', '3499', '5199', '34699', '29990', '5599', '4099', '2490', '27899', '10889', '13399', '4350', '2899', '18999', '37690']\n",
      "['22990', '12999', '6899', '6499', '27999', '33999', '49999', '1299', '7999', '2470', '4299', '17499', '6199', '5019', '9890', '13869', '17299', '47999', '3499', '5199', '34699', '29990', '5599', '4099', '2499', '27899', '10889', '13399', '4350', '2899', '18999', '37690']\n",
      "['15900', '25999', '3099', '29990', '29699', '49999', '40300', '9599', '5599', '3200', '8399', '9900', '9849', '180000', '3299', '7889', '3399', '8049', '2790', '34699', '3099', '7099', '3499', '13399', '6399', '37499', '34999', '15899', '8400', '13999', '269990', '31299']\n",
      "['15900', '25999', '3099', '29990', '29699', '49999', '40300', '9599', '5599', '3200', '8399', '9900', '9849', '180000', '3299', '7889', '3399', '8049', '2790', '34699', '3099', '7099', '3499', '13399', '6399', '37499', '34999', '15899', '8400', '13999', '269990', '31299']\n",
      "['14499', '4999', '15999', '38299', '25999', '8899', '149899', '7599', '8599', '5969', '6599', '1499', '59999', '2879', '13899', '10599', '125499', '13389', '8599', '1900', '5299', '62999', '7499', '29999', '32599', '10899', '18999', '1499', '7999', '37999', '4799', '4999']\n",
      "['14499', '4999', '15999', '38299', '25999', '8899', '14599', '7599', '8599', '5969', '6599', '1499', '59999', '2870', '13899', '10599', '125499', '13389', '8599', '1900', '5299', '62999', '7499', '29999', '32599', '10899', '18999', '1499', '7999', '37999', '4799', '4999']\n",
      "['29699', '5300', '15499', '42999', '6699', '10299', '3299', '7399', '1699', '1299', '11890', '5699', '15489', '6200', '62999', '2989', '29499', '18699', '53999', '16999', '8899', '5496', '11990', '15299', '699', '15399', '32599', '7799', '3090', '11900', '8790', '3950']\n",
      "['29699', '5300', '15499', '42999', '6699', '10299', '3299', '7399', '1699', '1299', '11890', '5699', '15489', '6200', '62999', '2989', '29499', '18699', '53999', '16999', '8899', '5496', '11990', '15299', '699', '15399', '32599', '7799', '3090', '11900', '8799', '3950']\n",
      "['26999', '10299', '12799', '1899', '1900', '12999', '34999', '14799', '29999', '3899', '10900', '1589', '9099', '9700', '10899', '8499', '29999', '6999', '18999', '8989', '9699', '3280', '5299', '15099', '2990', '3980', '3399', '1650', '5690', '6799', '10399', '2190']\n",
      "['26999', '10299', '12799', '1899', '1900', '12999', '34999', '14799', '29999', '3899', '10900', '1589', '9099', '9700', '10899', '8499', '29999', '6999', '18999', '8989', '9699', '3280', '5299', '15099', '2990', '3980', '3399', '1650', '5690', '6799', '10399', '2190']\n",
      "['13699', '16690', '5499', '28999', '2149', '39999', '11869', '15499', '1399', '17900', '112999', '8149', '21499', '4999', '6899', '8249', '55699', '27999', '6799', '10999', '8849', '22999', '1998', '16999', '2799', '2289', '7399', '10599', '12599', '4400', '1899', '5900']\n",
      "['13699', '16690', '5499', '28999', '2149', '39999', '11889', '15499', '1399', '17900', '112999', '8149', '21499', '4999', '6899', '8249', '55699', '27990', '6799', '10999', '8849', '22999', '1998', '16999', '2799', '2289', '7399', '10599', '12599', '4400', '1899', '5900']\n",
      "['5399', '13499', '39999', '31699', '5689', '9699', '8200', '6289', '17450', '72999', '6689', '1599', '6999', '15499', '2719', '11890', '1699', '15499', '11199', '5999', '3699', '4598', '11699', '9990', '6299', '72900', '11399', '22290', '149999', '19999', '17000', '21500']\n",
      "['5399', '13499', '39999', '31699', '5689', '9699', '8200', '6289', '17450', '72999', '6689', '4599', '6999', '15499', '2719', '11890', '1699', '15499', '11199', '5999', '3699', '4598', '11699', '9990', '6299', '72900', '11399', '2299', '149999', '19999', '17000', '21500']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(model, evaluation_loader, converter, opt)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m length_of_data \u001b[38;5;241m=\u001b[39m length_of_data \u001b[38;5;241m+\u001b[39m batch_size\n\u001b[0;32m----> 8\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m text_for_pred \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m     torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_size, opt\u001b[38;5;241m.\u001b[39mbatch_max_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m text_for_loss, length_for_loss \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m     13\u001b[0m     labels, batch_max_length\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mbatch_max_length\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation(model,valid_loader,converter,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5097016",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tensor \u001b[38;5;241m=\u001b[39m ToTensor()(img)\n\u001b[1;32m      5\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mAlignCollate_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NapoleonPractice/easy_ocr/train/dataset.py:265\u001b[0m, in \u001b[0;36mAlignCollate.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    264\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, batch)\n\u001b[0;32m--> 265\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_ratio_with_pad:  \u001b[38;5;66;03m# same concept with 'Rosetta' paper\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         resized_max_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgW\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms.v2 import ToTensor\n",
    "img = Image.open(\"/home/research/NapoleonPractice/data/dataset_val_100smpl/images/2266_image_106154.jpg\")\n",
    "tensor = ToTensor()(img)\n",
    "tensor = tensor.unsqueeze(-1)\n",
    "AlignCollate_valid(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['129']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/NapoleonPractice/.venv/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1\n",
    "text_for_pred = (torch.LongTensor(batch_size, config.batch_max_length + 1).fill_(0).to(device))\n",
    "preds = model(tensor.unsqueeze(0), text_for_pred)\n",
    "preds = preds.to(\"cpu\")\n",
    "# Calculate evaluation loss for CTC decoder.\n",
    "preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "_, preds_index = preds.max(2)\n",
    "preds_index = preds_index.view(-1)\n",
    "preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "print(preds_str)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
